{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPT/EdNTwCh0vZrjV+1Nh06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanandaraj5597/MLFlow/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4o4HRzlEsEi",
        "colab_type": "text"
      },
      "source": [
        "Welcome to Assignment 3. Firstly, mlflow needs to be installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdzQeNLBEnaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install mlflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ2OiI7BFX21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeTd8rt_JKtU",
        "colab_type": "code",
        "outputId": "c28e39f0-0ae2-424a-b414-bd56da936d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir -p /drive\n",
        "#umount /drive\n",
        "!mount --bind /content/drive/My\\ Drive /drive\n",
        "!mkdir -p /drive/ngrok-ssh\n",
        "!mkdir -p ~/.ssh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/drive’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB5Zzfv7hBW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install ssh tmux vim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYGdO6u2hU_n",
        "colab_type": "code",
        "outputId": "5edbbe1d-a45c-4e91-ae7f-b2c4aa2872be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title\n",
        "%%writefile /drive/ngrok-ssh/sshd_config\n",
        "#\t$OpenBSD: sshd_config,v 1.101 2017/03/14 07:19:07 djm Exp $\n",
        "\n",
        "# This is the sshd server system-wide configuration file.  See\n",
        "# sshd_config(5) for more information.\n",
        "\n",
        "# This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin\n",
        "\n",
        "# The strategy used for options in the default sshd_config shipped with\n",
        "# OpenSSH is to specify options with their default value where\n",
        "# possible, but leave them commented.  Uncommented options override the\n",
        "# default value.\n",
        "\n",
        "#Port 22\n",
        "#AddressFamily any\n",
        "#ListenAddress 0.0.0.0\n",
        "#ListenAddress ::\n",
        "\n",
        "#HostKey /etc/ssh/ssh_host_rsa_key\n",
        "#HostKey /etc/ssh/ssh_host_ecdsa_key\n",
        "#HostKey /etc/ssh/ssh_host_ed25519_key\n",
        "\n",
        "# Ciphers and keying\n",
        "#RekeyLimit default none\n",
        "\n",
        "# Logging\n",
        "#SyslogFacility AUTH\n",
        "#LogLevel INFO\n",
        "\n",
        "# Authentication:\n",
        "\n",
        "#LoginGraceTime 2m\n",
        "#PermitRootLogin prohibit-password\n",
        "#StrictModes yes\n",
        "#MaxAuthTries 6\n",
        "#MaxSessions 10\n",
        "\n",
        "#PubkeyAuthentication yes\n",
        "\n",
        "# Expect .ssh/authorized_keys2 to be disregarded by default in future.\n",
        "#AuthorizedKeysFile\t.ssh/authorized_keys .ssh/authorized_keys2\n",
        "\n",
        "#AuthorizedPrincipalsFile none\n",
        "\n",
        "#AuthorizedKeysCommand none\n",
        "#AuthorizedKeysCommandUser nobody\n",
        "\n",
        "# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts\n",
        "#HostbasedAuthentication no\n",
        "# Change to yes if you don't trust ~/.ssh/known_hosts for\n",
        "# HostbasedAuthentication\n",
        "#IgnoreUserKnownHosts no\n",
        "# Don't read the user's ~/.rhosts and ~/.shosts files\n",
        "#IgnoreRhosts yes\n",
        "\n",
        "# To disable tunneled clear text passwords, change to no here!\n",
        "#PasswordAuthentication yes\n",
        "#PermitEmptyPasswords no\n",
        "\n",
        "# Change to yes to enable challenge-response passwords (beware issues with\n",
        "# some PAM modules and threads)\n",
        "ChallengeResponseAuthentication no\n",
        "\n",
        "# Kerberos options\n",
        "#KerberosAuthentication no\n",
        "#KerberosOrLocalPasswd yes\n",
        "#KerberosTicketCleanup yes\n",
        "#KerberosGetAFSToken no\n",
        "\n",
        "# GSSAPI options\n",
        "#GSSAPIAuthentication no\n",
        "#GSSAPICleanupCredentials yes\n",
        "#GSSAPIStrictAcceptorCheck yes\n",
        "#GSSAPIKeyExchange no\n",
        "\n",
        "# Set this to 'yes' to enable PAM authentication, account processing,\n",
        "# and session processing. If this is enabled, PAM authentication will\n",
        "# be allowed through the ChallengeResponseAuthentication and\n",
        "# PasswordAuthentication.  Depending on your PAM configuration,\n",
        "# PAM authentication via ChallengeResponseAuthentication may bypass\n",
        "# the setting of \"PermitRootLogin without-password\".\n",
        "# If you just want the PAM account and session checks to run without\n",
        "# PAM authentication, then enable this but set PasswordAuthentication\n",
        "# and ChallengeResponseAuthentication to 'no'.\n",
        "UsePAM yes\n",
        "\n",
        "#AllowAgentForwarding yes\n",
        "AllowTcpForwarding yes\n",
        "#GatewayPorts no\n",
        "X11Forwarding yes\n",
        "#X11DisplayOffset 10\n",
        "#X11UseLocalhost yes\n",
        "#PermitTTY yes\n",
        "PrintMotd no\n",
        "#PrintLastLog yes\n",
        "#TCPKeepAlive yes\n",
        "#UseLogin no\n",
        "#PermitUserEnvironment no\n",
        "#Compression delayed\n",
        "#ClientAliveInterval 0\n",
        "#ClientAliveCountMax 3\n",
        "#UseDNS no\n",
        "#PidFile /var/run/sshd.pid\n",
        "#MaxStartups 10:30:100\n",
        "#PermitTunnel no\n",
        "#ChrootDirectory none\n",
        "#VersionAddendum none\n",
        "\n",
        "# no default banner path\n",
        "#Banner none\n",
        "\n",
        "# Allow client to pass locale environment variables\n",
        "AcceptEnv LANG LC_*\n",
        "\n",
        "# override default of no subsystems\n",
        "Subsystem\tsftp\t/usr/lib/openssh/sftp-server\n",
        "\n",
        "# Example of overriding settings on a per-user basis\n",
        "#Match User anoncvs\n",
        "#\tX11Forwarding no\n",
        "#\tAllowTcpForwarding no\n",
        "#\tPermitTTY no\n",
        "#\tForceCommand cvs server"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /drive/ngrok-ssh/sshd_config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8yZ5yK9he8q",
        "colab_type": "code",
        "outputId": "825ab816-da89-4048-cc49-ef5e7fad177f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile ~/.ssh/authorized_keys\n",
        "~/.ssh/id_rsa.pub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /root/.ssh/authorized_keys\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlqf_zhihsHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /drive/ngrok-ssh/sshd_config /etc/ssh/sshd_config\n",
        "!service ssh restart\n",
        "!mkdir -p ~/.ssh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8wZff8ZhzXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /drive/ngrok-ssh\n",
        "%cd /drive/ngrok-ssh\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip -u ngrok-stable-linux-amd64.zip\n",
        "!cp /drive/ngrok-ssh/ngrok /ngrok\n",
        "!chmod +x /ngrok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atbSvUe4iLPz",
        "colab_type": "code",
        "outputId": "f5cfa1f9-1d81-41f9-f9a4-cd3a9c27b9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!/ngrok authtoken 1bf3fczopcAZh5c5fu3AY9WB2Nn_3dgegCwKE6vJ4pLgyA2mT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNhTwmBzic9X",
        "colab_type": "code",
        "outputId": "c3eec91c-5cda-4116-9afe-340a13e5ccfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile /drive/ngrok-ssh/ssh.yml\n",
        "tunnels:\n",
        "  ssh:\n",
        "    proto: tcp\n",
        "    addr: 22"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /drive/ngrok-ssh/ssh.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b_du5VPii14",
        "colab_type": "code",
        "outputId": "09cdffb6-816b-491f-f82a-826c17942cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile /drive/ngrok-ssh/http5000.yml\n",
        "tunnels:\n",
        "  http5000:\n",
        "    proto: http\n",
        "    addr: 5000\n",
        "    inspect: false\n",
        "    bind_tls: true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /drive/ngrok-ssh/http5000.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CccplSVfir4d",
        "colab_type": "code",
        "outputId": "7685fd99-fa06-4076-f904-f3f1bd00bdce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile /drive/ngrok-ssh/run_ngrok.sh\n",
        "#!/bin/sh\n",
        "set -x\n",
        "/ngrok start --config ~/.ngrok2/ngrok.yml --config /drive/ngrok-ssh/ssh.yml --log=stdout --config /drive/ngrok-ssh/http5000.yml \"$@\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /drive/ngrok-ssh/run_ngrok.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yhx1epvi-CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "get_ipython().system_raw('bash /drive/ngrok-ssh/run_ngrok.sh ssh http5000 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_WXi_gjsZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import urllib.parse\n",
        "\n",
        "def get_ngrok_info():\n",
        "  return requests.get('http://localhost:4040/api/tunnels').json()\n",
        "\n",
        "def get_ngrok_tunnels():\n",
        "  for tunnel in get_ngrok_info()['tunnels']:\n",
        "    name = tunnel['name']\n",
        "    yield name, tunnel\n",
        "\n",
        "def get_ngrok_tunnel(name):\n",
        "  for name1, tunnel in get_ngrok_tunnels():\n",
        "    if name == name1:\n",
        "      return tunnel\n",
        "\n",
        "def get_ngrok_url(name, local=False):\n",
        "  if local:\n",
        "    return get_ngrok_tunnel(name)['config']['addr']\n",
        "  else:\n",
        "    return get_ngrok_tunnel(name)['public_url']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq65XLzvj0HM",
        "colab_type": "code",
        "outputId": "3c04c4d6-d96f-4f77-8de4-6cd504a3322a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for name, tunnel in get_ngrok_tunnels():\n",
        "  local_url = get_ngrok_url(name, local=True)\n",
        "  public_url = get_ngrok_url(name, local=False)\n",
        "  print('{:12s} {} <-> {}'.format(name, public_url, local_url))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ssh          tcp://0.tcp.ngrok.io:11205 <-> localhost:22\n",
            "http5000     https://322d4d47.ngrok.io <-> http://localhost:5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPVf0LdFpCSh",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to host our mlflow local host 1t port 5000 gloablly. Use the above mentioned http5000 link to get in.\n",
        "\n",
        "Now let's get started with our nerual network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlH9RhrspSJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "import mlflow\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtcsSHw4pWWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = torchvision.datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw1kw_umpmQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load the test data\n",
        "testset = torchvision.datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCqrxR1sp0hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(Net,self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1,16,3)\n",
        "      self.conv2 = nn.Conv2d(16,16,3)\n",
        "      self.fc1   = nn.Linear(24*24*16,100,bias=False)\n",
        "      self.fc2   = nn.Linear(100,10,bias=False)\n",
        "\n",
        "   def forward(self,x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1,24*24*16)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "my_net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(my_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "nn.init.xavier_uniform_(my_net.conv1.weight)\n",
        "nn.init.xavier_uniform_(my_net.conv2.weight)\n",
        "nn.init.xavier_uniform_(my_net.fc1.weight)\n",
        "nn.init.xavier_uniform_(my_net.fc2.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc0RlkFhp8N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Beginning Training...\")\n",
        "exp_id = mlflow.create_experiment('fashion_net_train')\n",
        "\n",
        "for epoch in range(5):\n",
        "   \n",
        "   correct_pred_count = 0\n",
        "   running_loss = 0.0\n",
        "   for i, data in enumerate(trainloader,0):\n",
        "      inputs, labels = data\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = my_net(inputs)\n",
        "      loss = criterion(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss\n",
        "\n",
        "      pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "      correct_prediction = accuracy_score(labels,pred_labels,normalize=False) \n",
        "\n",
        "      correct_pred_count += correct_prediction\n",
        "\n",
        "      if i%1000 == 999:\n",
        "         train_accuracy = (correct_pred_count/((i+1)*batch_size)) * 100\n",
        "         print('[%d, %5d] loss: %.3f train_accuracy: %.3f' % (epoch + 1, i + 1, running_loss / 1000,train_accuracy))\n",
        "         run_loss = running_loss/1000\n",
        "\n",
        "         with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "            mlflow.log_metric(\"loss\",float(run_loss))\n",
        "            mlflow.log_metric(\"Training_accuracy\",float(train_accuracy))\n",
        "\n",
        "         running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3D20erpucn5",
        "colab_type": "code",
        "outputId": "5fc6f8a7-de54-4e58-c2ec-d86bdad05939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"Started Testing...\")\n",
        "\n",
        "correct_pred_count = 0\n",
        "\n",
        "for i,data in enumerate(testloader,0):\n",
        "   inputs, labels = data\n",
        "\n",
        "   outputs = my_net(inputs)\n",
        "\n",
        "   pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "   correct_prediction = accuracy_score(labels,pred_labels,normalize=False)\n",
        "\n",
        "   correct_pred_count += correct_prediction\n",
        "\n",
        "   max_iter = i\n",
        "\n",
        "test_accuracy = (correct_pred_count/(batch_size*(max_iter+1))) * 100\n",
        "\n",
        "print ('Test_accuracy: %.3f' % (test_accuracy))\n",
        "\n",
        "print(\"Finished Testing!\\n\\nStarted loading the model...\")\n",
        "\n",
        "path = './fashion_net.pth'\n",
        "torch.save(my_net.state_dict(),path)\n",
        "\n",
        "print(\"Finished loading the model!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Testing...\n",
            "Test_accuracy: 90.480\n",
            "Finished Testing!\n",
            "\n",
            "Started loading the model...\n",
            "Finished loading the model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRMp6M-Ig7pb",
        "colab_type": "text"
      },
      "source": [
        "Now, lets get started with Step 2, varying the model parameters first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw2lWIERGJGD",
        "colab_type": "text"
      },
      "source": [
        "The below sub-routine is used for inference given a input testloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmhcKzHpF3fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(testloader):\n",
        "  #print(\"Started Testing...\")\n",
        "  correct_pred_count = 0\n",
        "\n",
        "  for i,data in enumerate(testloader,0):\n",
        "    inputs, labels = data\n",
        "\n",
        "    outputs = my_net(inputs)\n",
        "\n",
        "    pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "    correct_prediction = accuracy_score(labels,pred_labels,normalize=False)\n",
        "\n",
        "    correct_pred_count += correct_prediction\n",
        "\n",
        "    max_iter = i\n",
        "\n",
        "  test_accuracy = (correct_pred_count/(batch_size*(max_iter+1))) * 100\n",
        "\n",
        "  #print ('Test_accuracy: %.3f' % (test_accuracy))\n",
        "\n",
        "  #print(\"Finished Testing!\")\n",
        "\n",
        "  return test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt4N_UH_GJAK",
        "colab_type": "text"
      },
      "source": [
        "A run-time configurable neural network class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQ8kRAZhGfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self,number_of_filters1,number_of_filters2,filter1_dim,filter2_dim,num_outputs1):\n",
        "      super(Net,self).__init__()\n",
        "      self.fc1_input_dim = number_of_filters2*((28-filter1_dim+1-filter2_dim+1)*(28-filter1_dim+1-filter2_dim+1))\n",
        "      self.conv1 = nn.Conv2d(1,number_of_filters1,filter1_dim)\n",
        "      self.conv2 = nn.Conv2d(number_of_filters1,number_of_filters2,filter2_dim)\n",
        "      self.fc1   = nn.Linear((number_of_filters2*((28-filter1_dim+1-filter2_dim+1)*(28-filter1_dim+1-filter2_dim+1))),num_outputs1,bias=True)\n",
        "      self.fc2   = nn.Linear(num_outputs1,10,bias=True)\n",
        "\n",
        "   def forward(self,x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1,self.fc1_input_dim)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "train_time = 0\n",
        "\n",
        "for i in range(2):\n",
        "  filter1_dim = 5 + (i*2)\n",
        "  for j in range (2):\n",
        "    filter2_dim = 5 + (j*2)\n",
        "    for k in range(3):\n",
        "      number_of_filters1 = 16 + (k*2) + 2\n",
        "      for l in range(3):\n",
        "        number_of_filters2 = 16 + (l*2) + 2\n",
        "        for m in range(2):\n",
        "          num_outputs1 = 100 + (m+1)*10\n",
        "\n",
        "          my_net = Net(number_of_filters1,number_of_filters2,filter1_dim,filter2_dim,num_outputs1)\n",
        "          \n",
        "          criterion = nn.CrossEntropyLoss()\n",
        "          optimizer = optim.SGD(my_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "          nn.init.xavier_uniform_(my_net.conv1.weight)\n",
        "          nn.init.xavier_uniform_(my_net.conv2.weight)\n",
        "          nn.init.xavier_uniform_(my_net.fc1.weight)\n",
        "          nn.init.xavier_uniform_(my_net.fc2.weight)\n",
        "\n",
        "          experiment_name = \"fashion_net_\" + str(number_of_filters1) + \"_\" + str(filter1_dim) + \"_\" + str(number_of_filters2) + \"_\" + str(filter2_dim) + \"_\" + str(num_outputs1)\n",
        "\n",
        "          print(\"Beginning Training for \" + str(experiment_name))\n",
        "          exp_id = mlflow.create_experiment(experiment_name)\n",
        "          \n",
        "          for epoch in range (100):\n",
        "            correct_pred_count = 0\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            for _, data in enumerate(trainloader,0):\n",
        "              inputs, labels = data\n",
        "              \n",
        "              t = time.time()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              outputs = my_net(inputs)\n",
        "              loss = criterion(outputs,labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss += loss\n",
        "   \n",
        "              train_time += time.time() - t\n",
        "              pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "              correct_prediction = accuracy_score(labels,pred_labels,normalize=False) \n",
        "\n",
        "              correct_pred_count += correct_prediction\n",
        "\n",
        "            train_accuracy = (correct_pred_count/60000) * 100\n",
        "            run_loss = running_loss/60000\n",
        "            print('[%d] loss: %.3f train_accuracy: %.3f' % (epoch + 1, run_loss,train_accuracy))\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            test_accuracy = inference(testloader)\n",
        "\n",
        "            with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "              mlflow.log_metric(\"loss\",float(run_loss))\n",
        "              mlflow.log_metric(\"Training_epochs\",float(train_time))\n",
        "              mlflow.log_metric(\"Training_accuracy\",float(train_accuracy))\n",
        "              mlflow.log_metric(\"Testing_accuracy\",float(test_accuracy))\n",
        "\n",
        "              mlflow.log_param(\"Learning_rate\",0.001)\n",
        "              mlflow.log_param(\"Momentum\",0.9)\n",
        "\n",
        "            if train_accuracy >= 90:\n",
        "              print('Training time for this configuration is: '+str(train_time))\n",
        "              print('Test accuracy for this configuration is: '+str(test_accuracy))\n",
        "              train_time = 0\n",
        "              break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdNsgM8svF9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self,number_of_filters1,number_of_filters2,filter1_dim,filter2_dim,num_outputs1):\n",
        "      super(Net,self).__init__()\n",
        "      self.fc1_input_dim = number_of_filters2*((28-filter1_dim+1-filter2_dim+1)*(28-filter1_dim+1-filter2_dim+1))\n",
        "      self.conv1 = nn.Conv2d(1,number_of_filters1,filter1_dim)\n",
        "      self.conv2 = nn.Conv2d(number_of_filters1,number_of_filters2,filter2_dim)\n",
        "      self.fc1   = nn.Linear((number_of_filters2*((28-filter1_dim+1-filter2_dim+1)*(28-filter1_dim+1-filter2_dim+1))),num_outputs1,bias=True)\n",
        "      self.fc2   = nn.Linear(num_outputs1,10,bias=True)\n",
        "\n",
        "   def forward(self,x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1,self.fc1_input_dim)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "train_time = 0\n",
        "\n",
        "for k in range(4):\n",
        "  if k==0:\n",
        "    learning_rate = 0.01\n",
        "    momentum = 0.85\n",
        "  elif k==1:\n",
        "    learning_rate = 0.0008\n",
        "    momentum = 0.85\n",
        "  elif k==2:\n",
        "    learning_rate = 0.0008\n",
        "    momentum = 0.8\n",
        "  else:\n",
        "    learning_rate = 0.01\n",
        "    momentum = 0.8\n",
        "\n",
        "  for j in range (4):\n",
        "    if j==0:\n",
        "      batch_size = 10\n",
        "    elif j==1:\n",
        "      batch_size = 25\n",
        "    elif j==2:\n",
        "      batch_size = 50\n",
        "    else:\n",
        "      batch_size = 500\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "      my_net = Net(20,18,7,5,110)  #Best config from the previous experiment\n",
        "\n",
        "      init_id = i\n",
        "      if i==0:  #Identity matrix initialization\n",
        "       nn.init.kaiming_uniform_(my_net.conv1.weight)\n",
        "       nn.init.kaiming_uniform_(my_net.conv2.weight)\n",
        "       nn.init.kaiming_uniform_(my_net.fc1.weight)\n",
        "       nn.init.kaiming_uniform_(my_net.fc2.weight)\n",
        "      elif i==1: #Xavier normal initialization\n",
        "       nn.init.xavier_normal_(my_net.conv1.weight)\n",
        "       nn.init.xavier_normal_(my_net.conv2.weight)\n",
        "       nn.init.xavier_normal_(my_net.fc1.weight)\n",
        "       nn.init.xavier_normal_(my_net.fc2.weight)\n",
        "      else: #Default init by Pytorch\n",
        "       default = 1\n",
        "          \n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = optim.SGD(my_net.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "      experiment_name = \"fashion_net_\" + str(learning_rate) + \"_\" + str(momentum) + \"_\" + str(batch_size) + \"_\" + str(init_id)\n",
        "\n",
        "      print(\"Beginning Training for \" + str(experiment_name))\n",
        "      exp_id = mlflow.create_experiment(experiment_name)\n",
        "          \n",
        "      for epoch in range (100):\n",
        "        correct_pred_count = 0\n",
        "        running_loss = 0.0\n",
        "            \n",
        "        for _, data in enumerate(trainloader,0):\n",
        "          inputs, labels = data\n",
        "              \n",
        "          t = time.time()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = my_net(inputs)\n",
        "          loss = criterion(outputs,labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss\n",
        "   \n",
        "          train_time += time.time() - t\n",
        "          pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "          correct_prediction = accuracy_score(labels,pred_labels,normalize=False) \n",
        "\n",
        "          correct_pred_count += correct_prediction\n",
        "\n",
        "        train_accuracy = (correct_pred_count/60000) * 100\n",
        "        run_loss = running_loss/60000\n",
        "        print('[%d] loss: %.3f train_accuracy: %.3f' % (epoch + 1, run_loss,train_accuracy))\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        test_accuracy = inference(testloader)\n",
        "\n",
        "        with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "          mlflow.log_metric(\"loss\",float(run_loss))\n",
        "          mlflow.log_metric(\"Training_epochs\",float(train_time))\n",
        "          mlflow.log_metric(\"Training_accuracy\",float(train_accuracy))\n",
        "          mlflow.log_metric(\"Testing_accuracy\",float(test_accuracy))\n",
        "\n",
        "          mlflow.log_param(\"Learning_rate\",0.001)\n",
        "          mlflow.log_param(\"Momentum\",0.9)\n",
        "          mlflow.log_param(\"Batch_size\",float(batch_size))\n",
        "          mlflow.log_param(\"Initialization_method\",float(init_id))\n",
        "\n",
        "        if train_accuracy >= 90:\n",
        "          print('Training time for this configuration is: '+str(train_time))\n",
        "          print('Test accuracy for this configuration is: '+str(test_accuracy))\n",
        "          train_time = 0\n",
        "          break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PtvoTyRb5pf",
        "colab_type": "text"
      },
      "source": [
        "Step 3: Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSFZAS8Hb8Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score \n",
        "import copy\n",
        "import mlflow\n",
        "\n",
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "      super(Net,self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1,16,3)\n",
        "      self.conv2 = nn.Conv2d(16,16,3)\n",
        "      self.fc1   = nn.Linear(24*24*16,100,bias=True)\n",
        "      self.fc2   = nn.Linear(100,10,bias=True)\n",
        "\n",
        "   def forward(self,x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1,24*24*16)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = torchvision.datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = torchvision.datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def inference(testloader):\n",
        " print(\"Started Testing...\")\n",
        "\n",
        " correct_pred_count = 0\n",
        "\n",
        " for i,data in enumerate(testloader,0):\n",
        "    inputs, labels = data\n",
        "\n",
        "    outputs = my_net(inputs)\n",
        "\n",
        "    pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "    correct_prediction = accuracy_score(labels,pred_labels,normalize=False)\n",
        "\n",
        "    correct_pred_count += correct_prediction\n",
        "\n",
        "    max_iter = i\n",
        "\n",
        " test_accuracy = (correct_pred_count/(batch_size*(max_iter+1))) * 100\n",
        "\n",
        " print ('Test_accuracy: %.3f' % (test_accuracy))\n",
        "\n",
        " print(\"Finished Testing!\")\n",
        "\n",
        " return test_accuracy\n",
        "\n",
        "my_net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(my_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "nn.init.xavier_uniform_(my_net.conv1.weight)\n",
        "nn.init.xavier_uniform_(my_net.conv2.weight)\n",
        "nn.init.xavier_uniform_(my_net.fc1.weight)\n",
        "nn.init.xavier_uniform_(my_net.fc2.weight)\n",
        "\n",
        "break_flag = 0\n",
        "for epoch in range(100):\n",
        "   \n",
        "   correct_pred_count = 0\n",
        "   running_loss = 0.0\n",
        "   for i, data in enumerate(trainloader,0):\n",
        "      inputs, labels = data\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = my_net(inputs)\n",
        "      loss = criterion(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss\n",
        "\n",
        "      pred_out, pred_labels = torch.max(outputs,1)\n",
        "\n",
        "      correct_prediction = accuracy_score(labels,pred_labels,normalize=False) \n",
        "\n",
        "      correct_pred_count += correct_prediction\n",
        "\n",
        "      if i%1000 == 999:\n",
        "         train_accuracy = (correct_pred_count/((i+1)*batch_size)) * 100\n",
        "         print('[%d, %5d] loss: %.3f train_accuracy: %.3f' % (epoch + 1, i + 1, running_loss / 1000,train_accuracy))\n",
        "         run_loss = running_loss/1000\n",
        "\n",
        "         running_loss = 0.0\n",
        "\n",
        "         if train_accuracy >= 90:\n",
        "            break_flag = 1\n",
        "            break\n",
        "\n",
        "   if break_flag == 1:\n",
        "      break\n",
        "\n",
        "test_accuracy_prel = inference(testloader)\n",
        "\n",
        "print('Test accuracy prel is '+str(test_accuracy_prel))\n",
        "\n",
        "#torch.save(my_net.state_dict(),'./fashion_net.pth')\n",
        "my_net_prel = Net()\n",
        "my_net_prel = copy.deepcopy(my_net)\n",
        "\n",
        "prune_arr = []\n",
        "\n",
        "for i in range(16):\n",
        "  min_val = torch.sum(torch.abs(my_net.conv1.weight[0])).item()\n",
        "  min_idx = 0\n",
        "\n",
        "  for j in range (16-1): \n",
        "    if(min_val > torch.sum(torch.abs(my_net.conv1.weight[i+1])).item()) and (torch.sum(torch.abs(my_net.conv1.weight[i+1])).item() != 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.conv1.weight[i+1])).item()\n",
        "    if(min_val == 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.conv1.weight[i+1])).item()\n",
        "\n",
        "  print('Gonna prune layer '+str(min_idx))\n",
        "  my_net.conv1.weight[min_idx][0].detach()\n",
        "  my_net.conv1.bias[min_idx].detach()\n",
        "  my_net.conv1.bias[min_idx] = 0\n",
        "  my_net.conv1.weight[min_idx][0] = 0\n",
        "\n",
        "  prune_arr.append(min_idx)\n",
        "\n",
        "  my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "  my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "\n",
        "  for epoch in range(5):\n",
        "    print('Beginning training for this epoch')\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "     inputs, labels = data\n",
        "     \n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     outputs = my_net(inputs)\n",
        "     loss = criterion(outputs,labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     for idx in prune_arr:\n",
        "       my_net.conv1.weight[idx][0] = 0\n",
        "       my_net.conv1.bias[idx] = 0\n",
        "       my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "       my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "\n",
        "    print('Epoch done')\n",
        "\n",
        "    correct_pred_count = 0\n",
        "    for i,data in enumerate(testloader,0):\n",
        "      inputs, labels = data\n",
        "\n",
        "      outputs = my_net(inputs)\n",
        "      \n",
        "      pred_out, pred_labels = torch.max(outputs,1)\n",
        "      correct_prediction = accuracy_score(labels,pred_labels,normalize=False)\n",
        "      correct_pred_count += correct_prediction\n",
        "      max_iter = i\n",
        "      \n",
        "      test_accuracy = (correct_pred_count/(4*(max_iter+1))) * 100\n",
        "\n",
        "    print ('Test_accuracy: %.3f' % (test_accuracy))\n",
        "    print(\"Finished Testing!\")\n",
        "    \n",
        "    experiment_name = 'conv_layer1_prune' + str(epoch)\n",
        "    exp_id = mlflow.create_experiment(experiment_name)\n",
        "    with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "      mlflow.log_metric(\"Epoch\",float(epoch+1))\n",
        "      mlflow.log_metric(\"Prune_channel\",float(min_idx))\n",
        "      mlflow.log_metric(\"Test_accuracy\",float(test_accuracy))\n",
        "\n",
        "    if test_accuracy - test_accuracy_prel >= 1:\n",
        "      break\n",
        "\n",
        "  #test_accuracy = inference(testloader)\n",
        "\n",
        "  print('Test accuracy is '+str(test_accuracy))\n",
        "\n",
        "  if test_accuracy - test_accuracy_prel >= 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqnUaBd0djNe",
        "colab_type": "text"
      },
      "source": [
        "Going to prune convolution layer 2, FC layer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly0vJVoodpe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prune_arr_1 = []\n",
        "max_val = 0\n",
        "\n",
        "my_net = Net()\n",
        "my_net = copy.deepcopy(my_net_prel)\n",
        "\n",
        "optimizer = optim.SGD(my_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "test_accuracy_prel = inference(testloader)\n",
        "print('Before pruning '+str(test_accuracy_prel))\n",
        "\n",
        "for j in range(16):\n",
        "  min_val = torch.sum(torch.abs(my_net.conv2.weight[0])).item()\n",
        "  min_idx = 0\n",
        "\n",
        "  for i in range (16-1): \n",
        "    if(min_val > torch.sum(torch.abs(my_net.conv2.weight[i+1])).item()) and (torch.sum(torch.abs(my_net.conv2.weight[i+1])).item() != 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.conv2.weight[i+1])).item()\n",
        "    if(min_val == 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.conv2.weight[i+1])).item()\n",
        "\n",
        "  print('Gonna prune layer '+str(min_idx))\n",
        "  my_net.conv2.weight[min_idx].detach()\n",
        "  my_net.conv2.bias[min_idx].detach()\n",
        "  my_net.conv2.bias[min_idx] = 0\n",
        "  my_net.conv2.weight[min_idx] = 0\n",
        "\n",
        "  my_net.conv2.weight = torch.nn.parameter.Parameter(my_net.conv2.weight)\n",
        "  my_net.conv2.bias = torch.nn.parameter.Parameter(my_net.conv2.bias)\n",
        "\n",
        "  for idx in prune_arr:\n",
        "    my_net.conv1.weight[idx][0].detach()\n",
        "    my_net.conv1.bias[idx].detach()\n",
        "    my_net.conv1.bias[idx] = 0\n",
        "    my_net.conv1.weight[idx][0] = 0\n",
        "    my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "    my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "\n",
        "  prune_arr_1.append(min_idx)\n",
        "\n",
        "  for epoch in range(5):\n",
        "    print('Beginning training for this epoch')\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "     inputs, labels = data\n",
        "     \n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     outputs = my_net(inputs)\n",
        "     loss = criterion(outputs,labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     for idx in prune_arr:\n",
        "       my_net.conv1.weight[idx][0] = 0\n",
        "       my_net.conv1.bias[idx] = 0\n",
        "       my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "       my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "\n",
        "     for idx in prune_arr_1:\n",
        "       my_net.conv2.weight[idx] = 0\n",
        "       my_net.conv2.bias[idx] = 0\n",
        "       my_net.conv2.weight = torch.nn.parameter.Parameter(my_net.conv2.weight)\n",
        "       my_net.conv2.bias = torch.nn.parameter.Parameter(my_net.conv2.bias)\n",
        "\n",
        "    print('Epoch done')\n",
        "\n",
        "    test_accuracy = inference(testloader)\n",
        "\n",
        "    experiment_name = 'conv_layer2_prune' + str(epoch)\n",
        "    exp_id = mlflow.create_experiment(experiment_name)\n",
        "    with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "      mlflow.log_metric(\"Epoch\",float(epoch+1))\n",
        "      mlflow.log_metric(\"Prune_channel\",float(min_idx))\n",
        "      mlflow.log_metric(\"Test_accuracy\",float(test_accuracy))\n",
        "\n",
        "    if test_accuracy - test_accuracy_prel >= 1:\n",
        "      break\n",
        "\n",
        "  print('Test accuracy is '+str(test_accuracy))\n",
        "\n",
        "  if test_accuracy - test_accuracy_prel >= 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMwEDFXdvPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prune_arr_2 = []\n",
        "max_val = 0\n",
        "\n",
        "my_net = Net()\n",
        "my_net = copy.deepcopy(my_net_prel)\n",
        "\n",
        "print(my_net.conv1.weight.requires_grad)\n",
        "print(my_net_prel.conv1.weight.requires_grad)\n",
        "\n",
        "optimizer = optim.SGD(my_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "test_accuracy_prel = inference(testloader)\n",
        "print('Before pruning '+str(test_accuracy_prel))\n",
        "\n",
        "for j in range(100):\n",
        "  min_val = torch.sum(torch.abs(my_net.fc1.weight[0])).item()\n",
        "  min_idx = 0\n",
        "\n",
        "  for i in range (100-1): \n",
        "    if(min_val > torch.sum(torch.abs(my_net.fc1.weight[i+1])).item()) and (torch.sum(torch.abs(my_net.fc1.weight[i+1])).item() != 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.fc1.weight[i+1])).item()\n",
        "    if(min_val == 0):\n",
        "      min_idx = i+1\n",
        "      min_val = torch.sum(torch.abs(my_net.fc1.weight[i+1])).item()\n",
        "\n",
        "  print('Gonna prune layer '+str(min_idx))\n",
        "  my_net.fc1.weight[min_idx].detach()\n",
        "  my_net.fc1.bias[min_idx].detach()\n",
        "  my_net.fc1.bias[min_idx] = 0\n",
        "  my_net.fc1.weight[min_idx] = 0\n",
        "\n",
        "  my_net.fc1.weight = torch.nn.parameter.Parameter(my_net.fc1.weight)\n",
        "  my_net.fc1.bias = torch.nn.parameter.Parameter(my_net.fc1.bias)\n",
        "\n",
        "  for idx in prune_arr:\n",
        "    my_net.conv1.weight[idx][0].detach()\n",
        "    my_net.conv1.bias[idx].detach()\n",
        "    my_net.conv1.bias[idx] = 0\n",
        "    my_net.conv1.weight[idx][0] = 0\n",
        "    my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "    my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "  \n",
        "  for idx in prune_arr_1:\n",
        "    my_net.conv2.weight[idx].detach()\n",
        "    my_net.conv2.bias[idx].detach()\n",
        "    my_net.conv2.bias[idx] = 0\n",
        "    my_net.conv2.weight[idx] = 0\n",
        "    my_net.conv2.weight = torch.nn.parameter.Parameter(my_net.conv2.weight)\n",
        "    my_net.conv2.bias = torch.nn.parameter.Parameter(my_net.conv2.bias)\n",
        "\n",
        "  prune_arr_2.append(min_idx)\n",
        "\n",
        "  for epoch in range(5):\n",
        "    print('Beginning training for this epoch')\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "     inputs, labels = data\n",
        "     \n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     outputs = my_net(inputs)\n",
        "     loss = criterion(outputs,labels)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     for idx in prune_arr_2:\n",
        "       my_net.fc1.weight[idx]=0\n",
        "       my_net.fc1.bias[idx]=0\n",
        "       my_net.fc1.weight = torch.nn.parameter.Parameter(my_net.fc1.weight)\n",
        "       my_net.fc1.bias = torch.nn.parameter.Parameter(my_net.fc1.bias)\n",
        "\n",
        "     for idx in prune_arr:\n",
        "       my_net.conv1.weight[idx][0] = 0\n",
        "       my_net.conv1.bias[idx] = 0\n",
        "       my_net.conv1.weight = torch.nn.parameter.Parameter(my_net.conv1.weight)\n",
        "       my_net.conv1.bias = torch.nn.parameter.Parameter(my_net.conv1.bias)\n",
        "\n",
        "     for idx in prune_arr_1:\n",
        "       my_net.conv2.weight[idx] = 0\n",
        "       my_net.conv2.bias[idx] = 0\n",
        "       my_net.conv2.weight = torch.nn.parameter.Parameter(my_net.conv2.weight)\n",
        "       my_net.conv2.bias = torch.nn.parameter.Parameter(my_net.conv2.bias)\n",
        "\n",
        "    print('Epoch done')\n",
        "\n",
        "    test_accuracy = inference(testloader)\n",
        "\n",
        "    experiment_name = 'fc_layer1_prune' + str(epoch)\n",
        "    exp_id = mlflow.create_experiment(experiment_name)\n",
        "    with mlflow.start_run(experiment_id=exp_id,run_name='fashion_net_train'):\n",
        "      mlflow.log_metric(\"Epoch\",float(epoch+1))\n",
        "      mlflow.log_metric(\"Prune_channel\",float(min_idx))\n",
        "      mlflow.log_metric(\"Test_accuracy\",float(test_accuracy))\n",
        "\n",
        "    if test_accuracy - test_accuracy_prel >= 1:\n",
        "      break\n",
        "\n",
        "  print('Test accuracy is '+str(test_accuracy))\n",
        "\n",
        "  if test_accuracy - test_accuracy_prel >= 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu0hVH5yd60g",
        "colab_type": "text"
      },
      "source": [
        "Pruned Inference vs non-pruned inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i83o5gLJd-qm",
        "colab_type": "code",
        "outputId": "768d8ddb-aff1-48ca-a2e5-9414a9cd4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import time\n",
        "\n",
        "cum_time = 0\n",
        "\n",
        "inp = torch.rand(1,28,28)\n",
        "conv1_out = torch.rand(1,16,26,26)\n",
        "conv2_out = torch.rand(1,16,24,24)\n",
        "fc1_out = torch.rand(100)\n",
        "fc2_out = torch.rand(10)\n",
        "\n",
        "#Freeing the gradient buffers for inference\n",
        "my_net.conv1.weight.requires_grad = False\n",
        "my_net.conv2.weight.requires_grad = False\n",
        "my_net.fc1.weight.requires_grad = False\n",
        "my_net.fc2.weight.requires_grad = False\n",
        "\n",
        "my_net.conv1.bias.requires_grad = False\n",
        "my_net.conv2.bias.requires_grad = False\n",
        "my_net.fc1.bias.requires_grad = False\n",
        "my_net.fc2.bias.requires_grad = False\n",
        "\n",
        "for i in range (16):\n",
        "    for j in range(26):\n",
        "      for k in range(26):\n",
        "        conv1_out[0][i][j][k] = 0\n",
        "        for l in range(3):\n",
        "          for m in range(3):\n",
        "            t = time.time()\n",
        "            conv1_out[0][i][j][k] += my_net.conv1.weight[i][0][l][m] * inp[0][j+l][k+m]\n",
        "            conv1_out[0][i][j][k] += my_net.conv1.bias[i]\n",
        "            cum_time += time.time() - t\n",
        "\n",
        "for i in range (16):\n",
        "    for n in range(16):\n",
        "      for j in range(24):\n",
        "        for k in range(24):\n",
        "          conv2_out[0][i][j][k] = 0\n",
        "          for l in range(3):\n",
        "            for m in range(3):\n",
        "              t = time.time()\n",
        "              conv2_out[0][i][j][k] += my_net.conv2.weight[i][n][l][m] * conv1_out[0][n][j+l][k+m]\n",
        "              conv2_out[0][i][j][k] += my_net.conv2.bias[i]\n",
        "              cum_time += time.time() - t\n",
        "\n",
        "conv2_out_flattened = conv2_out.view(-1)\n",
        "\n",
        "for i in range(100):\n",
        "    fc1_out[i] = 0\n",
        "    for j in range(9216):\n",
        "      t = time.time()\n",
        "      fc1_out[i] += my_net.fc1.weight[i][j]*conv2_out_flattened[j]\n",
        "      fc1_out[i] += my_net.fc1.bias[i]\n",
        "      cum_time += time.time() - t\n",
        "\n",
        "for i in range(10):\n",
        "  fc2_out[i] = 0\n",
        "  for j in range(100):\n",
        "    t = time.time()\n",
        "    fc2_out[i] += my_net.fc2.weight[i][j]*fc1_out[j]\n",
        "    fc2_out[i] += my_net.fc2.bias[i]\n",
        "    cum_time += time.time() - t\n",
        "\n",
        "print('The inference time before pruning is: '+str(cum_time))\n",
        "\n",
        "cum_time = 0\n",
        "\n",
        "for i in range (16):\n",
        "  if i not in prune_arr:\n",
        "    for j in range(26):\n",
        "      for k in range(26):\n",
        "        conv1_out[0][i][j][k] = 0\n",
        "        for l in range(3):\n",
        "          for m in range(3):\n",
        "            t = time.time()\n",
        "            conv1_out[0][i][j][k] += my_net.conv1.weight[i][0][l][m] * inp[0][j+l][k+m]\n",
        "            conv1_out[0][i][j][k] += my_net.conv1.bias[i]\n",
        "            cum_time += time.time() - t\n",
        "\n",
        "for i in range (16):\n",
        "  if i not in prune_arr_1:\n",
        "    for n in range(16):\n",
        "      for j in range(24):\n",
        "        for k in range(24):\n",
        "          conv2_out[0][i][j][k] = 0\n",
        "          for l in range(3):\n",
        "            for m in range(3):\n",
        "              t = time.time()\n",
        "              conv2_out[0][i][j][k] += my_net.conv2.weight[i][n][l][m] * conv1_out[0][n][j+l][k+m]\n",
        "              conv2_out[0][i][j][k] += my_net.conv2.bias[i]\n",
        "              cum_time += time.time() - t\n",
        "\n",
        "conv2_out_flattened = conv2_out.view(-1)\n",
        "\n",
        "for i in range(100):\n",
        "  if i not in prune_arr_2:\n",
        "    fc1_out[i] = 0\n",
        "    for j in range(9216):\n",
        "      t = time.time()\n",
        "      fc1_out[i] += my_net.fc1.weight[i][j]*conv2_out_flattened[j]\n",
        "      fc1_out[i] += my_net.fc1.bias[i]\n",
        "      cum_time += time.time() - t\n",
        "\n",
        "for i in range(10):\n",
        "  fc2_out[i] = 0\n",
        "  for j in range(100):\n",
        "    t = time.time()\n",
        "    fc2_out[i] += my_net.fc2.weight[i][j]*fc1_out[j]\n",
        "    fc2_out[i] += my_net.fc2.bias[i]\n",
        "    cum_time += time.time() - t\n",
        "\n",
        "print('The inference time after pruning is: '+str(cum_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The inference time before pruning is: 95.12167453765869\n",
            "The inference time after pruning is: 91.91962003707886\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}